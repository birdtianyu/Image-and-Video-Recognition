{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "problem1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJc23_Bdo68j"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcD1FomFo68l"
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8iDXE4uo68m"
      },
      "source": [
        "### Extract, Transform, Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTvEJnXMo68m"
      },
      "source": [
        "# Extract and Transform\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root=r'./Dataset',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    root=r'./Dataset',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "# Load\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size = 100,\n",
        "    shuffle = True  # to have the data reshuffled at every epoch\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size = 100,\n",
        "    shuffle = False\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjFM6Yjqo68m",
        "outputId": "4040147f-9149-4b4b-ad47-36fa5f84e7d8"
      },
      "source": [
        "batch = next(iter(train_loader))\n",
        "images, labels = batch\n",
        "print(images.shape,'\\t',labels.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 1, 28, 28]) \t torch.Size([100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZRBeriBo68o"
      },
      "source": [
        "### Create Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs-Zfqiyo68o"
      },
      "source": [
        "class ConvNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=12*6*6, out_features=120)\n",
        "        # self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
        "        self.out = nn.Linear(in_features=120, out_features=10)\n",
        "        \n",
        "    def forward(self, d):\n",
        "        # conv1 layer\n",
        "        d = self.conv1(d)\n",
        "        d = nn.functional.relu(d)\n",
        "        d = nn.functional.max_pool2d(d, kernel_size=2, stride=2)\n",
        "    \n",
        "        # conv2 layer\n",
        "        d = self.conv2(d)\n",
        "        d = nn.functional.relu(d)\n",
        "        d = nn.functional.max_pool2d(d, kernel_size=2, stride=2)\n",
        "\n",
        "        # fully connected Layer1\n",
        "        # d = d.reshape(-1, 12*4*4)\n",
        "        d = d.flatten(start_dim=1)\n",
        "        d = self.fc1(d)\n",
        "        d = nn.functional.relu(d)\n",
        "\n",
        "        # fully connected Layer2\n",
        "        # d = self.fc2(d)\n",
        "        # d = nn.functional.relu(d)\n",
        "\n",
        "        # output layer\n",
        "        d = self.out(d)\n",
        "        # d = nn.functional.softmax(d)\n",
        "\n",
        "        return d"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBLQFRyso68o"
      },
      "source": [
        "network = ConvNetwork()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxubDUAdo68p"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwadaS8Uo68p"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu6xtzino68q"
      },
      "source": [
        "def test_model(model, loss_fn, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0.\n",
        "    num_correct = 0.\n",
        "    \n",
        "    for batch_idx, (x, y) in enumerate(test_loader):\n",
        "        y_pred = model(x)\n",
        "        loss += loss_fn(y_pred, y).item()\n",
        "        _, predicted = torch.max(y_pred.data, 1)\n",
        "        num_correct += (predicted == y).sum().item()\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    loss /= len(test_loader.dataset)\n",
        "    num_correct /= len(test_loader.dataset)\n",
        "    \n",
        "    return loss, num_correct"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVQLe-WSo68q"
      },
      "source": [
        "def get_num_correct(preds, labels):\n",
        "    '''\n",
        "    Calculate the number of correct predictions\n",
        "    '''\n",
        "    return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjri7vxfo68q",
        "outputId": "2a21ba46-d41f-4e0b-e82e-e6beeebbe729"
      },
      "source": [
        "# Learning Rate\n",
        "lr = 0.008\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(network.parameters(), lr=lr) \n",
        "\n",
        "# Training times\n",
        "for epoch in range(20):\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        images, labels = batch\n",
        "        \n",
        "        # forward\n",
        "        preds = network(images)\n",
        "        \n",
        "        # Compute the loss value.\n",
        "        loss = loss_function(preds, labels)\n",
        "        train_loss += loss.item()\n",
        "        train_correct += get_num_correct(preds, labels)\n",
        "        \n",
        "        # Update the weights\n",
        "        optimizer.zero_grad()  # Initialize to zero.\n",
        "        loss.backward()\n",
        "        optimizer.step()       # Updating the weights\n",
        "        \n",
        "        \n",
        "    # Compute the average loss and accuracy.\n",
        "    train_loss /= len(train_set)\n",
        "    train_correct /= float(len(train_set))\n",
        "    \n",
        "    # Evaluate the model on the test set.\n",
        "    test_loss, test_correct = test_model(network, loss_function, test_loader)\n",
        "    \n",
        "    # Report progress\n",
        "    print(f'#{epoch}: loss_test={test_loss:.4f}, loss_train={train_loss:.4f}, acc_test={test_correct:.4f}, acc_train={train_correct:.4f}')\n",
        "    \n",
        "\n",
        "# save model\n",
        "torch.save(network, './model/Model3.pkl')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#0: loss_test=0.0204, loss_train=0.0226, acc_test=0.5294, acc_train=0.2776\n",
            "#1: loss_test=0.0043, loss_train=0.0083, acc_test=0.8668, acc_train=0.7732\n",
            "#2: loss_test=0.0031, loss_train=0.0038, acc_test=0.9070, acc_train=0.8844\n",
            "#3: loss_test=0.0028, loss_train=0.0030, acc_test=0.9174, acc_train=0.9082\n",
            "#4: loss_test=0.0022, loss_train=0.0025, acc_test=0.9364, acc_train=0.9238\n",
            "#5: loss_test=0.0019, loss_train=0.0021, acc_test=0.9437, acc_train=0.9347\n",
            "#6: loss_test=0.0016, loss_train=0.0018, acc_test=0.9500, acc_train=0.9442\n",
            "#7: loss_test=0.0016, loss_train=0.0016, acc_test=0.9479, acc_train=0.9508\n",
            "#8: loss_test=0.0013, loss_train=0.0015, acc_test=0.9595, acc_train=0.9555\n",
            "#9: loss_test=0.0015, loss_train=0.0013, acc_test=0.9527, acc_train=0.9601\n",
            "#10: loss_test=0.0011, loss_train=0.0012, acc_test=0.9661, acc_train=0.9640\n",
            "#11: loss_test=0.0010, loss_train=0.0011, acc_test=0.9693, acc_train=0.9670\n",
            "#12: loss_test=0.0009, loss_train=0.0010, acc_test=0.9703, acc_train=0.9684\n",
            "#13: loss_test=0.0008, loss_train=0.0010, acc_test=0.9725, acc_train=0.9706\n",
            "#14: loss_test=0.0008, loss_train=0.0009, acc_test=0.9732, acc_train=0.9727\n",
            "#15: loss_test=0.0008, loss_train=0.0008, acc_test=0.9730, acc_train=0.9742\n",
            "#16: loss_test=0.0008, loss_train=0.0008, acc_test=0.9744, acc_train=0.9756\n",
            "#17: loss_test=0.0007, loss_train=0.0008, acc_test=0.9763, acc_train=0.9766\n",
            "#18: loss_test=0.0007, loss_train=0.0007, acc_test=0.9745, acc_train=0.9772\n",
            "#19: loss_test=0.0006, loss_train=0.0007, acc_test=0.9789, acc_train=0.9785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHXYOWDPo68r"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}